{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyzing Biorxiv Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Created: 6 May 2018*\n",
    "#### Author: Ali Sina Booeshaghi\n",
    "#### Summary: This notebook summarizes the efforts to analyze the Biorxiv Data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Loading the Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "## Cell to import all packages\n",
    "import json\n",
    "import os\n",
    "import operator\n",
    "import pprint\n",
    "import matplotlib.dates as mdates\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from dateutil import parser\n",
    "import pandas as pd\n",
    "from numpy import loadtxt\n",
    "# use the convention variable_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the smallest file, change data1.txt -> data21.txt for full data\n",
    "path_to_data = os.getcwd() + '/complete_data_2018_5_11/data/data1.txt'\n",
    "path_to_summary = os.getcwd() + '/complete_data_2018_5_11/analysis/journal_summary.txt'\n",
    "path_to_save = os.getcwd() + '/complete_data_2018_5_11/analysis'\n",
    "\n",
    "\n",
    "with open(path_to_data, 'rb') as f:\n",
    "    papers = json.load(f)\n",
    "\n",
    "with open(path_to_summary, 'rb') as f:\n",
    "    journal_summary = json.load(f)\n",
    "    \n",
    "# The way to access the data is as follows:\n",
    "# papers['papers'] gives you a list of all of the downloaded papers\n",
    "# papers['papers'][0] gives you the first one\n",
    "# papers['papers'][0][keyword] for keyword in ['abstract', 'authors', 'date', 'journal', 'link', 'text', 'title', 'twitter'] to access that info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Now lets look at some general information about the data we have**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of papers :  24598\n",
      "Number of journals :  1124\n",
      "Top ten journals by number of papers : \n",
      "[(u'Pre print', 15979),\n",
      " (u'Scientific Reports', 473),\n",
      " (u'eLife', 406),\n",
      " (u'PLOS ONE', 368),\n",
      " (u'Bioinformatics', 286),\n",
      " (u'Nature Communications', 229),\n",
      " (u'PNAS', 220),\n",
      " (u'PLOS Computational Biology', 211),\n",
      " (u'Genetics', 179),\n",
      " (u'PLOS Genetics', 176)]\n"
     ]
    }
   ],
   "source": [
    "expected_number_of_papers = 20500\n",
    "number_of_papers = len(papers['papers'])\n",
    "number_of_journals = len(journal_summary)\n",
    "journal_summary_sorted = sorted(journal_summary.items(), key=operator.itemgetter(1), reverse=True) # Sorts by most represented journal\n",
    "\n",
    "print \"Number of papers : \", number_of_papers\n",
    "print \"Number of journals : \", number_of_journals\n",
    "print \"Top ten journals by number of papers : \"\n",
    "pprint.pprint(journal_summary_sorted[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Here we have to now iterate through all of the data in papers to get richer information**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_author_list = []\n",
    "global_date_dict = {}\n",
    "author_network = []\n",
    "for paper_num in range(len(papers['papers'])):\n",
    "    abstract = papers['papers'][paper_num]['abstract']\n",
    "    authors  = papers['papers'][paper_num]['authors']\n",
    "    date     = papers['papers'][paper_num]['date']\n",
    "    journal  = papers['papers'][paper_num]['journal']\n",
    "    link     = papers['papers'][paper_num]['link']\n",
    "    text     = papers['papers'][paper_num]['text']\n",
    "    title    = papers['papers'][paper_num]['title']\n",
    "    twitter  = papers['papers'][paper_num]['twitter']\n",
    "    \n",
    "    author_network.append(authors)\n",
    "    # Getting a list of all of the authors\n",
    "    for dude in authors:\n",
    "        global_author_list.append(dude)\n",
    "    \n",
    "    # Getting a list of all of the dates the papers were posted\n",
    "    if date not in global_date_dict:\n",
    "        global_date_dict[date] = 1\n",
    "    else:\n",
    "        global_date_dict[date] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Author Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting author number counts\n",
    "duplicates =  len(global_author_list)\n",
    "global_author_tuple = [tuple(lst) for lst in global_author_list]\n",
    "global_author_list = list(set(global_author_tuple))\n",
    "\n",
    "# Checking out affiliations\n",
    "global_aff_dict = {}\n",
    "global_aff_list = []\n",
    "for dude in global_author_list:\n",
    "    name = dude[0]\n",
    "    aff = dude[2]\n",
    "    if aff in global_aff_dict:\n",
    "        # if the dude is aleady in the aff list\n",
    "        global_aff_dict[aff] += 1\n",
    "    else:\n",
    "        global_aff_dict[aff] = 1\n",
    "        \n",
    "    global_aff_list.append(aff)\n",
    "\n",
    "# NOTE: This sorts the instutitions by the number of unique authors form that institution, \n",
    "# TODO: I definitely double counted\n",
    "# TODO: The same thing but for the number of papers\n",
    "# TODO: fix numbers of authors\n",
    "\n",
    "global_aff_list = global_aff_dict.keys()\n",
    "len(global_aff_dict)\n",
    "global_aff_list_sorted = sorted(global_aff_dict.items(), key=operator.itemgetter(1), reverse=True) # Sorts by most represented journal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of authors :  140925\n",
      "Total number of authors who pub once :  121874\n",
      "Total number of authors who pub many :  19051\n",
      "Total number of publishing institutions :  32600\n"
     ]
    }
   ],
   "source": [
    "number_of_total_authors = len(global_author_list)\n",
    "number_of_authors_pub_many = duplicates - number_of_total_authors\n",
    "number_of_authors_pub_once = number_of_total_authors - number_of_authors_pub_many\n",
    "\n",
    "number_of_publishing_institutions = len(global_aff_list)\n",
    "\n",
    "print \"Total number of authors : \", number_of_total_authors\n",
    "print \"Total number of authors who pub once : \", number_of_authors_pub_once\n",
    "print \"Total number of authors who pub many : \", number_of_authors_pub_many\n",
    "print \"Total number of publishing institutions : \", number_of_publishing_institutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graph of Authors and Collaborators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *Creating Edge and Node List*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "trimmed_author_network = []\n",
    "for paper in author_network:\n",
    "    a_list = []\n",
    "    for author in paper:\n",
    "      a_list.append(author[0] + ', ' + author[1])  \n",
    "    trimmed_author_network.append(a_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create edge list\n",
    "edges = [[i[0],j] for i in trimmed_author_network for j in i[1:]]\n",
    "nodes = []\n",
    "for paper in trimmed_author_network:\n",
    "    for dude in paper:\n",
    "        if dude not in nodes:\n",
    "            nodes.append(dude)\n",
    "# check for duplicates\n",
    "len(nodes) != len(set(nodes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: \n",
      "Type: Graph\n",
      "Number of nodes: 118772\n",
      "Number of edges: 130208\n",
      "Average degree:   2.1926\n"
     ]
    }
   ],
   "source": [
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from networkx.drawing.nx_agraph import graphviz_layout, to_agraph\n",
    "## TODO: add identifiers to each node, ie institution\n",
    "def draw_graph(n, e):\n",
    "\n",
    "    # get nodes and edges\n",
    "    nodes = n\n",
    "    edges = e\n",
    "    \n",
    "    # create networkx graph\n",
    "    G=nx.Graph()\n",
    "\n",
    "    # add nodes\n",
    "    G.add_nodes_from(nodes)\n",
    "\n",
    "    # add edges\n",
    "    G.add_edges_from(edges)\n",
    "    print nx.info(G)\n",
    "\n",
    "    # draw graph\n",
    "    return G\n",
    "\n",
    "# draw example\n",
    "G = draw_graph(nodes, edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'Aull, Katherine H.', u'Gehring, Jase', u'Hendel, Nathan L.']"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "G.neighbors('Thomson, Matthew')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Date Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Working with dates\n",
    "dates, number_of_papers_posted = zip(*sorted(global_date_dict.items()))\n",
    "dates = list(dates)\n",
    "number_of_papers_posted = list(number_of_papers_posted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting number of papers posted over time\n",
    "# TODO: for plotting dates need to remove the once with errors ie DOI not found\n",
    "dates_dt = []\n",
    "for day in dates:\n",
    "    dates_dt.append(parser.parse(day))\n",
    "    \n",
    "dates_matplot = mdates.date2num(dates_dt)\n",
    "\n",
    "date_series = pd.Series(dates_dt, number_of_papers_posted)\n",
    "date_series.hist(bins=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Random Scripts that I don't really need anymore**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "## This scripts takes the summary document and produces a dictionary with journal, num of paper pairs. it uses the old summary.txt from /complete_data/data/pdfs/\n",
    "summary_dictionary = {}\n",
    "for n in summary:\n",
    "    new = n.split(':')\n",
    "    journal = new[0:-1][0]\n",
    "    ''.join(journal)\n",
    "    number_of_journal_papers = new[-1].strip('\\n')\n",
    "    summary_dictionary[journal] = int(number_of_journal_papers)\n",
    "\n",
    "with open(path_to_save + '/journal_summary.txt', 'w') as f:\n",
    "    json.dump(summary_dictionary, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
