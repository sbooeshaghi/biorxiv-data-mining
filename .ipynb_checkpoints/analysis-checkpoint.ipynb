{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This notebook details the analysis efforts undertaken by Sina Booeshaghi in analyzing the Biorxiv Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Date: 6 May 2018*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Loading the Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "## Cell to import all packages\n",
    "import json\n",
    "import os\n",
    "import operator\n",
    "import pprint\n",
    "import matplotlib.dates as mdates\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from dateutil import parser\n",
    "import pandas as pd\n",
    "# use the convention variable_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the smallest file, change data1.txt -> data21.txt for full data\n",
    "path_to_data = os.getcwd() + '/complete_data/data/data21.txt'\n",
    "path_to_summary = os.getcwd() + '/complete_data/analysis/journal_summary.txt'\n",
    "path_to_save = os.getcwd() + '/complete_data/analysis'\n",
    "\n",
    "\n",
    "with open(path_to_data, 'rb') as f:\n",
    "    papers = json.load(f)\n",
    "\n",
    "with open(path_to_summary, 'rb') as f:\n",
    "    journal_summary = json.load(f)\n",
    "    \n",
    "# The way to access the data is as follows:\n",
    "# papers['papers'] gives you a list of all of the downloaded papers\n",
    "# papers['papers'][0] gives you the first one\n",
    "# papers['papers'][0][keyword] for keyword in ['abstract', 'authors', 'date', 'journal', 'link', 'text', 'title', 'twitter'] to access that info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Now lets look at some general information about the data we have**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of papers :  20500\n",
      "Number of journals :  1112\n",
      "Top ten journals by number of papers : \n",
      "[(u'Pre print', 11820),\n",
      " (u'Scientific Reports', 469),\n",
      " (u'eLife', 400),\n",
      " (u'PLOS ONE', 364),\n",
      " (u'DOI Not Found', 310),\n",
      " (u'Bioinformatics', 284),\n",
      " (u'Nature Communications', 224),\n",
      " (u'PNAS', 218),\n",
      " (u'PLOS Computational Biology', 208),\n",
      " (u'Genetics', 179)]\n"
     ]
    }
   ],
   "source": [
    "expected_number_of_papers = 20500\n",
    "number_of_papers = len(papers['papers'])\n",
    "number_of_journals = len(journal_summary)\n",
    "journal_summary_sorted = sorted(journal_summary.items(), key=operator.itemgetter(1), reverse=True) # Sorts by most represented journal\n",
    "\n",
    "print \"Number of papers : \", number_of_papers\n",
    "print \"Number of journals : \", number_of_journals\n",
    "print \"Top ten journals by number of papers : \"\n",
    "pprint.pprint(journal_summary_sorted[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Here we have to now iterate through all of the data in papers to get richer information**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_author_list = []\n",
    "global_date_dict = {}\n",
    "for paper_num in range(len(papers['papers'])):\n",
    "    abstract = papers['papers'][paper_num]['abstract']\n",
    "    authors  = papers['papers'][paper_num]['authors']\n",
    "    date     = papers['papers'][paper_num]['date']\n",
    "    journal  = papers['papers'][paper_num]['journal']\n",
    "    link     = papers['papers'][paper_num]['link']\n",
    "    text     = papers['papers'][paper_num]['text']\n",
    "    title    = papers['papers'][paper_num]['title']\n",
    "    twitter  = papers['papers'][paper_num]['twitter']\n",
    "    \n",
    "    # Getting a list of all of the authors\n",
    "    for dude in authors:\n",
    "        global_author_list.append(dude)\n",
    "    \n",
    "    # Getting a list of all of the dates the papers were posted\n",
    "    if date not in global_date_dict:\n",
    "        global_date_dict[date] = 1\n",
    "    else:\n",
    "        global_date_dict[date] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Author Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting author number counts\n",
    "duplicates =  len(global_author_list)\n",
    "global_author_tuple = [tuple(lst) for lst in global_author_list]\n",
    "global_author_list = list(set(global_author_tuple))\n",
    "\n",
    "# Checking out affiliations\n",
    "global_aff_dict = {}\n",
    "\n",
    "for dude in global_author_list:\n",
    "    aff = dude[2]\n",
    "    if aff in global_aff_dict:\n",
    "        global_aff_dict[aff] += 1\n",
    "    else:\n",
    "        global_aff_dict[aff] = 1\n",
    "        \n",
    "    global_aff_list.append(aff)\n",
    "\n",
    "# NOTE: This sorts the instutitions by the number of unique authors form that institution, \n",
    "# TODO: I definitely double counted\n",
    "# TODO: The same thing but for the number of papers\n",
    "# TODO: fix numbers of authors\n",
    "\n",
    "global_aff_list = global_aff_dict.keys()\n",
    "len(global_aff_dict)\n",
    "global_aff_list_sorted = sorted(global_aff_dict.items(), key=operator.itemgetter(1), reverse=True) # Sorts by most represented journal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of authors :  115370\n",
      "Total number of authors who pub once :  99757\n",
      "Total number of authors who pub many :  15613\n",
      "Total number of publishing institutions :  26973\n"
     ]
    }
   ],
   "source": [
    "number_of_total_authors = len(global_author_list)\n",
    "number_of_authors_pub_many = duplicates - number_of_total_authors\n",
    "number_of_authors_pub_once = number_of_total_authors - number_of_authors_pub_many\n",
    "\n",
    "number_of_publishing_institutions = len(global_aff_list)\n",
    "\n",
    "print \"Total number of authors : \", number_of_total_authors\n",
    "print \"Total number of authors who pub once : \", number_of_authors_pub_once\n",
    "print \"Total number of authors who pub many : \", number_of_authors_pub_many\n",
    "print \"Total number of publishing institutions : \", number_of_publishing_institutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Date Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Working with dates\n",
    "dates, number_of_papers_posted = zip(*sorted(global_date_dict.items()))\n",
    "dates = list(dates)\n",
    "number_of_papers_posted = list(number_of_papers_posted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Unknown string format",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-231-c1220e5a7212>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mdates_dt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mday\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdates\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mdates_dt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mday\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mdates_matplot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmdates\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdate2num\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdates_dt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/sinabooeshaghi/software/anaconda/lib/python2.7/site-packages/dateutil/parser.pyc\u001b[0m in \u001b[0;36mparse\u001b[0;34m(timestr, parserinfo, **kwargs)\u001b[0m\n\u001b[1;32m   1166\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparserinfo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimestr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1167\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1168\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mDEFAULTPARSER\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimestr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1169\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/sinabooeshaghi/software/anaconda/lib/python2.7/site-packages/dateutil/parser.pyc\u001b[0m in \u001b[0;36mparse\u001b[0;34m(self, timestr, default, ignoretz, tzinfos, **kwargs)\u001b[0m\n\u001b[1;32m    557\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    558\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mres\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 559\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Unknown string format\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    560\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    561\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Unknown string format"
     ]
    }
   ],
   "source": [
    "# Plotting number of papers posted over time\n",
    "# TODO: for plotting dates need to remove the once with errors ie DOI not found\n",
    "dates_dt = []\n",
    "for day in dates:\n",
    "    dates_dt.append(parser.parse(day))\n",
    "    \n",
    "dates_matplot = mdates.date2num(dates_dt)\n",
    "\n",
    "date_series = pd.Series(dates_dt, number_of_papers_posted)\n",
    "date_series.hist(bins=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Random Scripts that I don't really need anymore**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "## This scripts takes the summary document and produces a dictionary with journal, num of paper pairs. it uses the old summary.txt from /complete_data/data/pdfs/\n",
    "summary_dictionary = {}\n",
    "for n in summary:\n",
    "    new = n.split(':')\n",
    "    journal = new[0:-1][0]\n",
    "    ''.join(journal)\n",
    "    number_of_journal_papers = new[-1].strip('\\n')\n",
    "    summary_dictionary[journal] = int(number_of_journal_papers)\n",
    "\n",
    "with open(path_to_save + '/journal_summary.txt', 'w') as f:\n",
    "    json.dump(summary_dictionary, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
